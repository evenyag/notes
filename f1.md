# F1: A Distributed SQL Database That Scales

# INTRODUCTION
F1 是一个具备容灾能力的，全球分布式数据库，兼具 OLTP 和 OLAP 能力，用于替换 Google 的 AdWords 广告系统中分库分表的 MySQL 系统

设计目标
- 可伸缩性 (scalability)，扩容要容易，加机器就行，同时做到业务无感知
- 可用性 (availability)，不管是数据中心宕机了，或者日常维护，修改 schema 等服务都需要保证可用（广告系统是核心业务，不可用会造成资金损失）
- 一致性 (consistency)，提供 ACID 事务，保证数据一致性和正确性
- 易用性 (usability)，提供完整的 SQL 查询支持以及其他用户需要的 SQL 数据库的功能，例如二级索引， ad hoc 查询

F1 在 Spanner 基础上，增加了若干特性
- 支持分布式 SQL 查询，包括和外部数据源做 join 的能力
- 事务一致的二级索引
- 异步 schema 变更
- 乐观事务
- 变更历史自动记录和发布

F1 的一些设计选择会导致常规读写的延迟变高，因此还开发了相关的技术来隐藏延迟，保证最终用户感受到的延时和原来基于 MySQL 的系统差不多
- 通过 F1 表的分级结构以及结构化的列数据类型支持，显式控制数据聚集的方式，提高数据聚集度，减少查询的 rpc 次数
- 重度使用 batching ，并行和异步读取

# BASIC ARCHITECTURE
基本架构如下

![architecture](pics/f1/architecture.png)

各个组件的交互
- 用户通过 F1 client 库和 F1 服务交互，其他工具底下也是基于这个库
- F1 server 负责读写远端数据，协调查询执行
- F1 client 和 load balancer 会优先连接附近 datacenter 的 F1 server
- F1 server 一般和存储该数据的 spanner 服务分布到相同的数据中心里，而 spanner 的数据在 CFS (Colossus File System) 上
    - CFS 和 spanner 不同，不是跨数据中心复制的，因此 spanner 不会和其他数据中心的 CFS 交互
- F1 server 基本上是无状态的
    - 客户端每个请求都可以和不同的 F1 server 交互
    - 只要需要进行悲观事务时，需要持有锁，才需要和一个 F1 server 保持连接直到事务结束
- 当查询发现分布式执行收益更大，则 planner 会选择分布式查询来提高并发度，降低延迟
    - 共享的 slave pool 里的进程可以像常规 F1 server 一样执行分布式查询计划的一个子计划
- 也支持将大规模的数据处理放到 MapReduce 上跑
- 由于数据需要在多个数据中心间做同步复制，因此提交的延时一般比较高，在 50 ~ 150 ms 左右

## Spanner
Spanner 是几乎和 F1 同步开发的底层存储系统，两个团队是紧密协作的

Spanner 提供了类似 directory ，层级化的数据模型。每个目录里的数据会进一步划分成 fragment ，通过 paxos group 做复制。每个 group 可以包含若干个 fragment

Spanner 在多个 paxos group 间支持基于二阶段提交的分布式事务，不过参与组数在 100 个之后中断的频率和延迟会极大增加

Spanner 提供了一个全局的安全时间戳（global safe timestamp），保证没有正在执行或者将来的事务有比他更小的提交时间戳，通常比当前时间落后 5 到 10 秒。基于这个时间戳的读操作可以在任何副本上执行，包括只读副本，而不会阻塞当前正在执行的事务

关于 spanner 可以进一步了解对应的论文

# DATA MODEL
## Hierarchical Schema
F1 的数据模型和 Spanner 的十分相似。实际上，原来 Spanner 的数据模型和 Bigtable 类似，但后来 Spanner 采用了 F1 的数据模型

整个模型是一个树形的结构，父表和子表的行是交织聚集在一起的，子表必须要有一个外键是父表主键的一部分前缀，类似下图的例子

![data model](pics/f1/data-model.png)

在 root 表中的一行就叫做 root 行，所有属于某个 root 行的子表的行都会和 root 行存到一个 spanner 目录下，这样带来的好处包括
- 相关的数据放到了一起，可以一次 rpc 就全部获取，有利于降低整体的延迟
- 更新 root row 相关的数据往往只需要操作一个目录，避免发起 2PC (二阶段提交)

实际上，这种组织方式并不是强制的，用户也可以采用类似 MySQL 那种组织方式，具体还是要看收益

## Protocol Buffers
F1 列的数据格式支持结构化数据，其实就是作为一个 blob 存储，但是支持使用 protocol buffer 的编码格式
- 方便应用层使用结构化数据，不需要自己实现一套将列数据映射到结构体的代码
- 有 repeated fields 支持，一些需要使用字表管理多条子记录的场景可以改为用 repeated fields 来实现，也可以避免 join 等开销

很多 F1 的表的 schema 可能就包含一个 pb 列

## Indexing
F1 的所有索引都是事务的，一致的，作为一个单独的表存放到 spanner 上，其 key 就是 index 的 key 拼上被索引的表的主键
- index key 可以是标量的列类型，也可以从 pb 列的字段中提取（包括 repeated）字段
- 有两种形式， local 和 global

local index 的 key 必须包含 root row 的主键作为前缀，存储时类似子表，会存到 root row 相同的目录下，它的更新开销很小

global index 不需要用 root row 的主键作为前缀，因此也就不能和要被索引的行分布到一起。对 global index 的更新需要通过 2PC 来实现，因此开销会大很多。索引一次更新太多，涉及到 2PC 的事务参与者也会多很多，效率会变低，也容易出错，因此用得比较少。如果要用，会建议写入方写入数据时，将数据拆成很多个小事务

# SCHEMA CHANGES
F1 的 schema 变更是完全异步化的，要实现这一点挑战是很大的
- F1 是分布式的，节点规模非常大
- 每个 F1 server 内存里会缓存 schema ，很难保证一次全部更新掉
- 变更过程中查询和事务也一直在执行
- 系统可用性和延迟不能受影响

因此， F1 的 schema 变更是异步应用的，逐步地在不同时刻应用到不同的 F1 server 。这意味着两台 F1 server 可能同时使用不同的 schema 更新数据库

这有可能会导致数据不一致或者损坏，例如加索引后，一个机器看到了新的 schema 并插入了数据，同时也写入了索引，而删除的请求却发到了有老的 schema 的服务器，就导致索引表被漏删

为了避免这种情况， F1 实现了以下算法
- 限制所有 F1 server 同时最多只能有两套不同的 schema ，要么使用当前的，要么使用下一个 schema 。为 schema 分配 lease ，lease 过期后就不能有服务器再使用对应的 schema
- 将一次 schema change 分成若干步，每步都保证互相兼容，不会导致异常情况，例如在前面的例子里
    - F1 先增加索引，但限制只能删除，不插入索引数据
    - 随后再升级为可以执行全部写操作
    - 随后启动一个 MapRedcue 任务回写所有行的索引
    - 最后令索引可见，可读

# TRANSACTIONS
F1 基于 spanner 实现了以下三种事务
- Snapshot 事务，使用固定的 snapshot timestamp 执行只读事务（可以 reading repeatable data）。默认会用 global safe timestamp ，落后 5-10 秒。用户也可以自己选择 timestamp 或者让 spanner 选择当前时间戳，但可能会导致延迟更高。 SQL 查询和 MapReduce 默认用这种事务
- 悲观事务，直接映射为 Spanner 的事务，使用有状态的协议，负责处理请求的 F1 server 会持有锁（读也需要），如果这个服务器重启了，事务会中断
- 乐观事务，它包含一个可以任意长的只读阶段，不需要加锁，和一个短的写阶段（Spanner 悲观事务）。F1 在读请求返回的每行数据中都有一个叫 lock 的隐藏列，包含这一行上次修改的时间，每次 commit 会自动设置为 commit timestamp ，在最后的写阶段在客户端里用于校验是否有冲突

F1 客户端默认用乐观事务，因为有以下好处
- 可以容忍行为不正常的客户端，因为读的时候不需要加锁，有些异常客户端可能会跑一个很长的事务
- 支持长周期的事务，也方便 debug ，因为一般悲观事务空闲超过 10s 就会被 kill 掉，以释放锁
- 可以在服务端做重试
- 状态在客户端维护，方便服务端做 failover
- 可以实现 speculative write ，先在外部发起一个乐观事务（例如在 MapReduce 里）读取数据，记录时间戳，随后可以再次用这些值和时间戳发起一次乐观事务，只有在中途没发生过写入时写入才成功

乐观事务也有以下缺点
- 幻写（Inert phantoms），因为新数据是没有 commit timestamp 的，所以可能看到新插入的数据，不过可以通过更粗粒度的锁（parent-table locks）来避免这个问题
- 高冲突时吞吐低

## Flexible Locking Granularity
F1 默认提供行粒度的锁，每行都有一个默认的 lock 列来保护该行的所有列。用户也可以自己定义额外的 lock 列来提高行操作的并发度，减少锁冲突，即一个 lock 列只包含该行的一部分列

同样，用户也可以用父表的列作为锁，来降低并发度，前面提到的幻写问题可以通过这个方式解决

# 参考
- [fuzhe1989 的博客](https://fuzhe1989.github.io/2020/12/22/f1-a-distributed-sql-database-that-scales/)
